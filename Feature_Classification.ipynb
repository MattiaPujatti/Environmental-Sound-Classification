{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "import pydub\n",
    "import numpy as np\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets previously created\n",
    "features = pd.read_csv(\"features.csv\", index_col=0)\n",
    "more_features = pd.read_csv(\"more_features.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipsClassifier():\n",
    "    \"\"\"The purpose of this class is to collect all the necessary steps and functions to construct a classification\n",
    "    model for our clips. \n",
    "    In particular, all the necessary steps to prepare the input dataset for the training process will \n",
    "    be implemented:\n",
    "    * standardization\n",
    "    * PCA\n",
    "    * One Hot Encoding\n",
    "    * train test split\n",
    "    Then a k-fold cross validation can be made in order to test several combination of hyperparameters\n",
    "    without constructing directly a validation set.\n",
    "    In the end, the performances will be shown in term of accuracy/loss also over different macro-categories\n",
    "    to finally quantify the quality of the model constructed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"Initialize some global parameters.\n",
    "        Dataset is a pandas dataframe with several \"features\" columns and one \"label\" column, \n",
    "        that contains the data that we want to fit.\"\"\"\n",
    "        \n",
    "        self.data = dataset        \n",
    "        self.Setup_Classifier()\n",
    "        \n",
    "        self.setup_completed = False\n",
    "        \n",
    "        self.best_model = None\n",
    "        self.hyperparameters_tuning_dict = {}\n",
    "        self.nested_scores = []\n",
    "        \n",
    "        self.confusion_matrix = None\n",
    "        \n",
    "        \n",
    "    def Setup_Classifier(self, pca_percentage=0.99, n_folds=5, n_jobs=-1, verbose=2,\n",
    "                         scaler_method='standard', encoder_method='onehot'):\n",
    "        \"\"\"Change the value of some parameters/methods used during data pre-processing and training step.\"\"\"\n",
    "        \n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.scaler = scaler_method\n",
    "        self.encoder = encoder_method\n",
    "        self.pca_percentage = pca_percentage\n",
    "        self.n_folds = n_folds\n",
    "        \n",
    "        # The sequent analysis will be performed via nested cross validation\n",
    "        self.inner_cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        self.outer_cv = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "\n",
    "    def _Setup_Data(self):\n",
    "        \"\"\"Performs standardization, PCA and label encoding.\"\"\"\n",
    "        \n",
    "        # Standardize data\n",
    "        if self.scaler == 'standard':\n",
    "            std_data = StandardScaler().fit_transform(self.data.drop(['label'], axis=1))\n",
    "        elif self.scaler == 'minmax':\n",
    "            std_data = MinMaxScaler(feature_range=(-1,1)).fit_transform(self.data.drop(['label'], axis=1))\n",
    "        else:\n",
    "            print('Invalid value of the scaler. Available: standard, minmax')\n",
    "            return\n",
    "        \n",
    "        # Let's apply the PCA keeping just a percentage of the information\n",
    "        if (self.pca_percentage>0) and (self.pca_percentage<1):\n",
    "            self.pca = PCA().fit(std_data)\n",
    "            self.cev = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "            pca_data = PCA(n_components=np.argmax(self.cev>self.pca_percentage)).fit_transform(std_data)\n",
    "        else:\n",
    "            pca_data = std_data\n",
    "\n",
    "        # Encode the labels\n",
    "        if self.encoder == 'onehot':\n",
    "            labels = OneHotEncoder(sparse=False).fit_transform(self.data[['label']].to_numpy())\n",
    "        elif self.encoder == 'label':\n",
    "            labels = LabelEncoder().fit_transform(self.data['label'])\n",
    "        else:\n",
    "            print('Invalid value of the encoder. Available: onehot, label')\n",
    "            return\n",
    "        \n",
    "        self.setup_completed = True\n",
    "        self.X = pca_data\n",
    "        self.Y = labels\n",
    "        \n",
    "        \n",
    "    def PCA_Variance_Ratio(self):\n",
    "        \"\"\"When PCA option is active, it plot the comulative sum of the variance ratio, in order\n",
    "        to represent the amount of information stored in the first principal components.\"\"\"\n",
    "        \n",
    "        if not self.setup_completed: self._Setup_Data()\n",
    "        \n",
    "        plt.plot(self.cev, color='red', lw=3, label='cev')\n",
    "        plt.axvline(np.argmax(cev>self.pca_percentage), ls='--', c='black', lw=1, \n",
    "                    label='cev = {}'.format(round(self.cev[np.argmax(cev>0.95)], 2)))\n",
    "        plt.xlabel('Number of components')\n",
    "        plt.ylabel('Cumulative explained variance')\n",
    "        plt.title('Study on the number of principal components')\n",
    "        plt.legend();\n",
    "\n",
    "        \n",
    "    def Run_Grid_Search(self, model, parameters):\n",
    "        \"\"\"Because of the small amount of data available, the analysis will be performed running a \n",
    "        nested cross validation over the clip set. For tuning the hyperparameters an \"inner\" K-fold\n",
    "        splitting will be defined to check which combination works better over the dataset.\"\"\"\n",
    "       \n",
    "        # Standardize, encode and eventually apply pca on the dataset\n",
    "        if not self.setup_completed: self._Setup_Data()\n",
    "        \n",
    "        # Non_nested parameter search and scoring\n",
    "        clf = GridSearchCV(estimator=model, param_grid=parameters, n_jobs=self.n_jobs, \n",
    "                           verbose=self.verbose, cv=self.inner_cv)\n",
    "        clf.fit(self.X, self.Y)\n",
    "        self.hyperparameters_tuning_dict = clf.cv_results_\n",
    "        self.best_model = clf.best_estimator_\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Optimal set of hyperparameters: \")\n",
    "            print(clf.best_params_)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def Run_Cross_Validation(self, model=None):\n",
    "        \"\"\"Because of the small amount of data available, the analysis will be performed running a \n",
    "        nested cross validation over the clip set. To estimate the performances of a model, an \"outer\" \n",
    "        K-fold splitting will be defined in order to compute the effective generalized accuracy as \n",
    "        the average of the validation values obtained among various folds.\n",
    "        Because of the stochastic nature of the approach, it may be better to repeat several times the \n",
    "        run to check if the results are compatible between themselves.\"\"\"\n",
    "        \n",
    "        # Standardize, encode and eventually apply pca on the dataset\n",
    "        if not self.setup_completed: self._Setup_Data()\n",
    "            \n",
    "        # If not specified, run the validation for the best model found by the grid search\n",
    "        if model is None: model = self.best_model\n",
    "        \n",
    "        # Nested CV cross validation\n",
    "        self.nested_scores = cross_val_score(model, X=self.X, y=self.Y, n_jobs=self.n_jobs, \n",
    "                                             verbose=2, cv=self.outer_cv)\n",
    "        best_accuracy = np.mean(self.nested_scores)\n",
    "        \n",
    "        if self.verbose > 0:\n",
    "            print(\"Average final accuracy estimated: {}%\".format(round(best_accuracy*100, 2)))  \n",
    "            \n",
    "    # Define another function macro accuracy, that computes ypred via cross_val_predict and you\n",
    "    # convert them to macro\n",
    "        \n",
    "        \n",
    "    def Compute_Confusion_Matrix(self, model=None):\n",
    "        \"\"\"Compute the confusion matrix according to the input model or the best one found by a \n",
    "        previous grid search.\"\"\"\n",
    "        \n",
    "        # Standardize, encode and eventually apply pca on the dataset\n",
    "        if not self.setup_completed: self._Setup_Data()\n",
    "            \n",
    "        # If not specified, run the validation for the best model found by the grid search\n",
    "        if model is None: model = self.best_model\n",
    "        \n",
    "        if self.confusion_matrix is None:\n",
    "            y_pred = cross_val_predict(model, self.X, self.Y, cv=self.outer_cv)\n",
    "            self.confusion_matrix = confusion_matrix(self.Y, y_pred)\n",
    "            \n",
    "        return self.confusion_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_performances = {'random_forest':{}, 'multi_layer_perceptron':{}, 'k_neighbors_classifier':{},\n",
    "                           'support_vector_machine':{}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    4.2s remaining:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'max_samples': 0.5, 'n_estimators': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    4.2s remaining:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 36.55%\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    7.6s remaining:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    9.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'bootstrap': True, 'max_features': 'sqrt', 'max_samples': 0.5, 'n_estimators': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    7.7s remaining:   11.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 52.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    9.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    9.7s finished\n"
     ]
    }
   ],
   "source": [
    "params_RF = {'n_estimators': [500, 1000],\n",
    "             'bootstrap': [True, False],\n",
    "             'max_samples' : [0.5, None],\n",
    "             'max_features': ['sqrt']}\n",
    "\n",
    "params_RF = {'n_estimators': [500],\n",
    "             'bootstrap': [True],\n",
    "             'max_samples' : [0.5],\n",
    "             'max_features': ['sqrt']}\n",
    "\n",
    "# RandomForest underperform with One Hot Encoding, so you need to change to LabelEncoder\n",
    "rf_cc = ClipsClassifier(dataset = features)\n",
    "rf_cc.Setup_Classifier(encoder_method='label', verbose=1)\n",
    "rf_cc.Run_Grid_Search(model = RandomForestClassifier(), parameters = params_RF)\n",
    "rf_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['random_forest']['features'] = rf_cc.nested_scores\n",
    "\n",
    "rf_cc = ClipsClassifier(dataset = more_features)\n",
    "rf_cc.Setup_Classifier(encoder_method='label', verbose=1)\n",
    "rf_cc.Run_Grid_Search(model = RandomForestClassifier(), parameters = params_RF)\n",
    "rf_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['random_forest']['more_features'] = rf_cc.nested_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   10.2s remaining:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   12.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'activation': 'relu', 'hidden_layer_sizes': 512, 'learning_rate_init': 0.01, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   11.6s remaining:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 23.75%\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   13.9s remaining:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'activation': 'relu', 'hidden_layer_sizes': 512, 'learning_rate_init': 0.01, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   12.7s remaining:   19.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 43.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.0s finished\n"
     ]
    }
   ],
   "source": [
    "params_MLP = {'hidden_layer_sizes':[128, 256, 512],\n",
    "             'activation':['logistic', 'relu'],\n",
    "             'solver':['sgd', 'adam'],\n",
    "              'learning_rate_init':[0.01, 0.001]}\n",
    "\n",
    "params_MLP = {'hidden_layer_sizes':[512],\n",
    "             'activation':['relu'],\n",
    "             'solver':['adam'],\n",
    "              'learning_rate_init':[0.01]}\n",
    "\n",
    "mlp_cc = ClipsClassifier(dataset = features)\n",
    "mlp_cc.Run_Grid_Search(model = MLPClassifier(), parameters = params_MLP)\n",
    "mlp_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['multi_layer_perceptron']['features'] = mlp_cc.nested_scores\n",
    "\n",
    "mlp_cc = ClipsClassifier(dataset = more_features)\n",
    "mlp_cc.Run_Grid_Search(model = MLPClassifier(), parameters = params_MLP)\n",
    "mlp_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['multi_layer_perceptron']['more_features'] = mlp_cc.nested_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 2, 'weights': 'distance'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 28.75%\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'algorithm': 'auto', 'leaf_size': 10, 'n_neighbors': 2, 'weights': 'distance'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 46.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "params_KNC = {'n_neighbors':[2,5,8,10],\n",
    "             'weights':['uniform', 'distance'],\n",
    "             'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "             'leaf_size':[10, 30, 50, 100]}\n",
    "\n",
    "params_KNC = {'n_neighbors':[2],\n",
    "             'weights':['distance'],\n",
    "             'algorithm':['auto'],\n",
    "             'leaf_size':[10]}\n",
    "\n",
    "# RandomForest underperform with One Hot Encoding, so you need to change to LabelEncoder\n",
    "knc_cc = ClipsClassifier(dataset = features)\n",
    "knc_cc.Run_Grid_Search(model = KNeighborsClassifier(), parameters = params_KNC)\n",
    "knc_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['k_neighbors_classifier']['features'] = knc_cc.nested_scores\n",
    "\n",
    "knc_cc = ClipsClassifier(dataset = more_features)\n",
    "knc_cc.Run_Grid_Search(model = KNeighborsClassifier(), parameters = params_KNC)\n",
    "knc_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['k_neighbors_classifier']['more_features'] = knc_cc.nested_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'C': 1, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.6s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 38.15%\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal set of hyperparameters: \n",
      "{'C': 0.1, 'kernel': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.5s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final accuracy estimated: 61.25%\n"
     ]
    }
   ],
   "source": [
    "params_SVM = {'C':[0.1, 0.5, 1],\n",
    "             'kernel':['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "# SVM doesn't work with One Hot Encoding, so you need to change to LabelEncoder\n",
    "svm_cc = ClipsClassifier(dataset = features)\n",
    "svm_cc.Setup_Classifier(encoder_method='label', verbose=1)\n",
    "svm_cc.Run_Grid_Search(model = SVC(), parameters = params_SVM)\n",
    "svm_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['support_vector_machine']['features'] = svm_cc.nested_scores\n",
    "\n",
    "svm_cc = ClipsClassifier(dataset = more_features)\n",
    "svm_cc.Setup_Classifier(encoder_method='label', verbose=1)\n",
    "svm_cc.Run_Grid_Search(model = SVC(), parameters = params_SVM)\n",
    "svm_cc.Run_Cross_Validation()\n",
    "\n",
    "classifiers_performances['support_vector_machine']['more_features'] = svm_cc.nested_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_forest': {'features': array([0.375 , 0.34  , 0.365 , 0.37  , 0.3775]),\n",
       "  'more_features': array([0.5175, 0.52  , 0.53  , 0.5325, 0.5475])},\n",
       " 'multi_layer_perceptron': {'features': array([0.245 , 0.2075, 0.2525, 0.245 , 0.2375]),\n",
       "  'more_features': array([0.425, 0.42 , 0.44 , 0.435, 0.45 ])},\n",
       " 'k_neighbors_classifier': {'features': array([0.3025, 0.2825, 0.295 , 0.265 , 0.2925]),\n",
       "  'more_features': array([0.4725, 0.4775, 0.4725, 0.4525, 0.4675])},\n",
       " 'support_vector_machine': {'features': array([0.36  , 0.3975, 0.3625, 0.3875, 0.4   ]),\n",
       "  'more_features': array([0.6025, 0.66  , 0.6125, 0.595 , 0.5925])}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features': array([0.375 , 0.34  , 0.365 , 0.37  , 0.3775]), 'more_features': array([0.5175, 0.52  , 0.53  , 0.5325, 0.5475])}\n",
      "{'features': array([0.245 , 0.2075, 0.2525, 0.245 , 0.2375]), 'more_features': array([0.425, 0.42 , 0.44 , 0.435, 0.45 ])}\n",
      "{'features': array([0.3025, 0.2825, 0.295 , 0.265 , 0.2925]), 'more_features': array([0.4725, 0.4775, 0.4725, 0.4525, 0.4675])}\n",
      "{'features': array([0.36  , 0.3975, 0.3625, 0.3875, 0.4   ]), 'more_features': array([0.6025, 0.66  , 0.6125, 0.595 , 0.5925])}\n"
     ]
    }
   ],
   "source": [
    "for i in classifiers_performances.values():print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Undefined type for flatten: <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-596ad555b3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers_performances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-a793787ba61b>\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-a793787ba61b>\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-a793787ba61b>\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Undefined type for flatten: %s\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Undefined type for flatten: <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "def flatten_dict(d):\n",
    "    def items():\n",
    "        for key, value in d.items():\n",
    "            if isinstance(value, dict):\n",
    "                for subkey, subvalue in flatten_dict(value).items():\n",
    "                    yield key + \".\" + subkey, subvalue\n",
    "            else:\n",
    "                yield key, value\n",
    "\n",
    "    return dict(items())\n",
    "flatten(classifiers_performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.set_ylim(0,1)\n",
    "\n",
    "for method in classifiers_performances.values():\n",
    "    sns.boxplot(y=method['features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "for label in classifiers_performances_df['index']:\n",
    "    sns.boxplot(x=label, y=classifiers_performances_df[classifiers_performances_df['index']==label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "sns.boxplot(x=1, y=classifiers_performances['random_forest']['more_features'], ax=ax)\n",
    "sns.boxplot(x=2, y=classifiers_performances['multi_layer_perceptron']['more_features'], ax=ax)\n",
    "sns.boxplot(y=classifiers_performances['k_neighbors_classifier']['more_features'], ax=ax)\n",
    "sns.boxplot(y=classifiers_performances['support_vector_machine']['more_features'], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
